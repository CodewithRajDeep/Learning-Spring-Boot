spring.application.name=mcp-client
spring.ai.mcp.client.enabled=true

spring.ai.mcp.client.name= my-ollama-mcp-client
spring.ai.mcp.client.version=1.0.3

spring.ai.mcp.client.type=SYNC


spring.ai.mcp.client.request-timeout=20s


spring.ai.mcp.client.toolback.enabled=true

spring.ai.mcp.client.stdio.connections.ollama-server.command = ollama
spring.ai.mcp.client.stdio.connections.ollama-server.args=run,llama2
spring.ai.ollama.base-url=http://localhost:11434

# The Ollama model to use
spring.ai.ollama.chat.model=llama2 # or other models like qwen, mistral, etc.

# Whether to download the model if it's not present locally (when_missing, never)
spring.ai.ollama.chat.options.model-download=when_missing

# Other Ollama chat options (optional)
# spring.ai.ollama.chat.options.temperature=0.7
# spring.ai.ollama.chat.options.top-p=0.9